"use strict";
// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
Object.defineProperty(exports, "__esModule", { value: true });
exports.ArrowRecordBatchTableRowTransform = exports.ArrowRecordBatchTransform = exports.ArrowRecordReaderTransform = exports.ArrowRawTransform = void 0;
const stream_1 = require("stream");
const apache_arrow_1 = require("apache-arrow");
/**
 * ArrowRawTransform implements a node stream Transform that reads
 * ReadRowsResponse from BigQuery Storage Read API and convert
 * a raw Arrow Record Batch.
 */
class ArrowRawTransform extends stream_1.Transform {
    constructor() {
        super({
            readableObjectMode: false,
            writableObjectMode: true,
        });
    }
    _transform(response, _, callback) {
        var _a;
        if (!(response.arrowRecordBatch &&
            response.arrowRecordBatch.serializedRecordBatch)) {
            callback(null);
            return;
        }
        callback(null, (_a = response.arrowRecordBatch) === null || _a === void 0 ? void 0 : _a.serializedRecordBatch);
    }
}
exports.ArrowRawTransform = ArrowRawTransform;
/**
 * ArrowRecordReaderTransform implements a node stream Transform that reads
 * a byte stream of raw Arrow Record Batch and convert to a stream of Arrow
 * RecordBatchStreamReader.
 */
class ArrowRecordReaderTransform extends stream_1.Transform {
    constructor(session) {
        super({
            objectMode: true,
        });
        this.session = session;
    }
    _transform(serializedRecordBatch, _, callback) {
        var _a;
        const buf = Buffer.concat([
            (_a = this.session.arrowSchema) === null || _a === void 0 ? void 0 : _a.serializedSchema,
            serializedRecordBatch,
        ]);
        const reader = apache_arrow_1.RecordBatchReader.from(buf);
        callback(null, reader);
    }
}
exports.ArrowRecordReaderTransform = ArrowRecordReaderTransform;
/**
 * ArrowRecordBatchTransform implements a node stream Transform that reads
 * a RecordBatchStreamReader and convert a stream of Arrow RecordBatch.
 */
class ArrowRecordBatchTransform extends stream_1.Transform {
    constructor() {
        super({
            objectMode: true,
        });
    }
    _transform(reader, _, callback) {
        const batches = reader.readAll();
        for (const row of batches) {
            this.push(row);
        }
        callback(null);
    }
}
exports.ArrowRecordBatchTransform = ArrowRecordBatchTransform;
/**
 * ArrowRecordBatchTableRowTransform implements a node stream Transform that reads
 * an Arrow RecordBatch and convert a stream of BigQuery TableRow.
 */
class ArrowRecordBatchTableRowTransform extends stream_1.Transform {
    constructor() {
        super({
            objectMode: true,
        });
    }
    _transform(batch, _, callback) {
        const rows = new Array(batch.numRows);
        for (let i = 0; i < batch.numRows; i++) {
            rows[i] = {
                f: new Array(batch.numCols),
            };
        }
        for (let j = 0; j < batch.numCols; j++) {
            const column = batch.selectAt([j]);
            const field = column.schema.fields[0];
            const columnName = field.name;
            for (let i = 0; i < batch.numRows; i++) {
                const fieldData = column.get(i);
                const fieldValue = fieldData === null || fieldData === void 0 ? void 0 : fieldData.toJSON()[columnName];
                rows[i].f[j] = {
                    v: convertArrowValue(fieldValue, field.type),
                };
            }
        }
        for (let i = 0; i < batch.numRows; i++) {
            this.push(rows[i]);
        }
        callback(null);
    }
}
exports.ArrowRecordBatchTableRowTransform = ArrowRecordBatchTableRowTransform;
function convertArrowValue(fieldValue, type) {
    if (fieldValue === null) {
        return null;
    }
    if (apache_arrow_1.DataType.isList(type)) {
        const arr = fieldValue.toJSON();
        return arr.map((v) => {
            // Arrays/lists in BigQuery have the same datatype for every element
            // so getting the first one is all we need
            const elemType = type.children[0].type;
            return { v: convertArrowValue(v, elemType) };
        });
    }
    if (apache_arrow_1.DataType.isStruct(type)) {
        const tableRow = {};
        Object.keys(fieldValue).forEach(key => {
            const elemType = type.children.find(f => f.name === key);
            if (!tableRow.f) {
                tableRow.f = [];
            }
            tableRow.f.push({
                v: convertArrowValue(fieldValue[key], elemType === null || elemType === void 0 ? void 0 : elemType.type),
            });
        });
        return tableRow;
    }
    if (apache_arrow_1.DataType.isTimestamp(type)) {
        // timestamp comes in microsecond, convert to nanoseconds
        // to make it compatible with BigQuery.timestamp.
        return fieldValue * 1000;
    }
    return fieldValue;
}
//# sourceMappingURL=arrow_transform.js.map